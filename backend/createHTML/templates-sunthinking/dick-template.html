{% extends "base-template.html" %} {% block content %}
<main>
  <section class="desktop-header">
    <div class="info">
      <a href="index.html#dick" class="article-home-button"><< home</a>
      <div class="article-type-container">
 __  ___  ___  __                    ___    
/__`  |  |__  |__) |__|  /\  |\ | | |__     
.__/  |  |___ |    |  | /~~\ | \| | |___                                       
 __     __                                  
|  \ | /  ` |__/                            
|__/ | \__, |  \                                                         
           </div>

      <div class="col-text-body">
        <h3>Artificial Intelligence</h3>
        <p><i>by <a href="https://hdsr.mitpress.mit.edu/user/stephanie-dick">Stephanie Dick</a></i></p>
        
        <p><i>This essay was initially published in the Harvard Data Science Review  on Jul 01, 2019.</i></p>

        <p>There is a plaque at Dartmouth College that reads: “In this building during the summer of 1956
            John McCarthy (Dartmouth College), Marvin L. Minsky (MIT), Nathaniel Rochester (IBM), and Claude
            Shannon (Bell Laboratories) conducted the Dartmouth Summer Research Project on Artificial
            Intelligence. First use of the term ‘Artificial Intelligence.’ Founding of Artificial
            Intelligence as a research discipline ‘to proceed on the basis of the conjecture that every
            aspect of learning or any other feature of intelligence can in principle be so precisely
            described that a machine can be made to simulate it.’” The plaque was hung in 2006, in
            conjunction with a conference commemorating the 50th anniversary of the Summer Research Project,
            and it enshrines the standard account of the history of Artificial Intelligence-that it was born
            in 1955 when these veterans of early military computing applied to the Rockefeller Foundation
            for a summer grant to fund the workshop that in turn shaped the field. The plaque also cites the
            core conjecture of their proposal: that intelligent human behavior consisted in processes that
            could be formalized and reproduced in a machine (McCarthy, Minksy, Rochester, & Shannon, 1955).
        </p>

        <p>Grounded in the postwar traditions of systems engineering and cybernetics, and drawing from the
            longer history of mathematical logic and philosophy aimed at formal descriptions of human
            thinking, they held that cognitive faculties could be abstracted from the supporting physical
            operations of the brain. Thus the former could, in principle, be reproduced in different
            material substrata so long as the formal rules could be executed there (Kline, 2011, 2015). Two
            of the attendees, Herbert Simon and Allen Newell, influentially proposed more specifically that
            human minds and modern digital computers were ‘species of the same genus,’ namely <em>symbolic
                information processing systems;</em> both take symbolic information as input, manipulate it
            according
            to a set of formal rules, and in so doing can solve problems, formulate judgments, and make
            decisions (Crowther-Heyck, 2008; Heyck, 2005; Newell & Simon, 1972). After the 1956 workshop,
            this became the dominant approach, and artificial intelligence researchers accordingly set out
            to identify the formal processes that constituted intelligent human behavior in medical
            diagnosis, chess, mathematics, language processing, and so on, in hopes of reproducing that
            behavior by automated means.
        </p>

        <p>Overwhelmingly, however, artificial intelligence today resembles this symbolic approach in name
            only. Most key commitments and approaches were abandoned over the course of the twentieth
            century. Perhaps most notably, <em>human intelligence</em> was the central exemplar around which
            early
            automation attempts were oriented. The goal was to reproduce intelligent human behavior in
            machines by uncovering the processes at work in our own intelligence such that they could be
            automated. Today, however, most researchers want to design automated systems that perform well
            in complex problem domains by <em>any</em> means, rather than by <em>human-like</em> means
            (Floridi, 2016). In
            fact, many powerful approaches today set out intentionally to bypass human behavior, as in the
            case of automated game-playing systems that develop impressive strategies entirely by playing
            only against themselves, keeping track of what moves are more likely to produce a win, rather
            than by deploying human-inspired heuristics or training through play with human experts (Pollack
            & Blair, 1997; Tesauro, 1995). That the core project could have changed so dramatically
            highlights the fact that what counts as <em>intelligence</em> is a moving target in the history
            of
            artificial intelligence.
        </p>

        <p>That research communities picked out different behaviors and processes as constitutive of
            intelligence is actually also highlighted in the early history itself. Standard historical
            accounts of artificial intelligence often overstate the significance of the Dartmouth workshop
            and the symbolic approach associated with it. Indeed, even according to the participants
            themselves, the workshop was something of a disappointment. McCarthy recollected that “anybody
            who was there was pretty stubborn about pursuing the ideas that he had before he came, nor was
            there, as far as I could see, any real exchange of ideas” (McCorduck, 2004, p. 114). McCarthy’s
            lamentation also hints at the fact that approaches to artificial intelligence research were more
            multifaceted than accounts of “good old-fashioned AI” (as symbolic artificial intelligence was
            dubbed in the 1980s) might suggest.
        </p>

        <p>For example, proponents of a field called ‘expert systems’ rejected the premise that human
            intelligence was grounded in rule-bound reasoning alone. They believed, in part because of the
            consistent disappointment attendant to that approach, that human intelligence depended on what
            experts <em>know</em> and not just how they think (Brock, 2018; Collins, 1990; Feigenbaum, 1977;
            Forsythe, 2002)<em>.</em> Edward Feigenbaum (1977), the Stanford-based computer scientist who
            named this
            field, proposed that:
        </p>

        <p>
            <blockquote>We must hypothesize from our experience to date that the problem-solving power
                exhibited in an
                intelligent agent’s performance is primarily a consequence of the specialist’s
                <em>knowledge</em>
                employed by the agent, and only very secondarily related to the generality and power of the
                inference method employed. Our agents must be knowledge-rich, even if they are methods-poor.
                (p.
                3, emphasis added)</blockquote>
        </p>

        <p>In this approach, ‘knowledge engineers’ would interview human experts, observe their
            problem-solving practices, and so on, in hopes of eliciting and making explicit what they knew
            such that it could be encoded for automated use (Feigenbaum, 1977, p. 4). Expert systems offered
            a different explanation of human intelligence, and their own theory of knowledge, revealing that
            both were moving targets in this early research.
        </p>

        <p>Still others, many of whom were interested in automated pattern recognition, focused on attempts
            not to simulate the human <em>mind</em> but to artificially reproduce the synapses of the
            <em>brain</em> in
            ‘artificial neural networks.’ Neural networks themselves date from the 1940s and 50s, and were
            originally meant to simulate brain synapses by digital means (Jones, 2018). These neural
            networks, now largely stripped of all but the most cursory relationship to human brains, are at
            work in many of today’s powerful machine learning systems, emphasizing yet again the protean
            character of ‘intelligent behavior’ in this history.
        </p>

        <p>The history of artificial intelligence is, therefore, not just the history of mechanical attempts
            to replicate or replace some static notion of human intelligence, but also a changing account of
            how we think about intelligence itself. In that respect, artificial intelligence wasn’t born at
            Dartmouth in 1955, as the standard account would have us believe, but rather participates in
            much longer histories of what counts as intelligence and what counts as artificial. For example,
            essays in Phil Husbands, Owen Holland, and Michael Wheeler’s <em>The Mechanical Mind in
                History</em>
            (2008) situate symbolic information processing at the <em>end</em> of a long history of
            mechanical
            theories of mind.
        </p>

        <p>Artificial intelligence belongs in the history of <em>human intelligence</em>, in ways that
            complicate
            teleological accounts in which symbolic AI emerge naturally and inevitably from attempts across
            centuries to reduce human reasoning to a logical formalism. Human cognitive faculties have been
            theorized, partitioned, valued, and devalued in different ways at different times. For example,
            in the 18th and early 19th centuries, complex arithmetic calculations were associated with
            virtuosic mathematical genius (Daston, 1994). Famous mathematicians the likes of Gauss and
            Laplace were celebrated in their time for their “lightning arithmetic.” But by the
            mid-nineteenth century, calculation had largely lost its association with genius, and was
            “drifting from the neighborhood of intelligence to that of something very like its opposite”
            (Daston, 1994, p. 186). Calculation was demoted to the “merely mechanical” or the “automatic,”
            requiring no real presence of mind or cognitive attention. Historian Lorraine Daston tracks this
            shift within the efforts of French mathematician Gaspard de Prony, who applied Adam Smith’s
            principles of the division of labor to mathematical calculation, breaking down complex
            calculations into small and simple arithmetic operations that could be executed (indeed executed
            best) by uneducated people. The “deskilling” of calculation and the demotion of this cognitive
            faculty precipitated a shift in the demography of calculators; once the purview of
            <em>philosophes</em>
            and mathematicians, calculation became in the 19th century the domain of unskilled laborers and
            of women. In large numbers, these “human computers” (indeed, for most of its history, the work
            ‘computer’ referred to a person rather than to a machine), could carry out complex calculations
            by aggregating their arithmetic labor. Once demoted to the realm of the “merely mechanical,”
            calculation was also ripe for machine automation, and mathematicians like Charles Babbage and
            Charles Thomas de Colmar set out to design machines that could do for mental labor what factory
            automation did for physical labor (Schaffer, 1994). Daston (1994) uses this historical
            transformation to argue that “what intelligence meant and- still more telling- who had it
            shifted in tandem with the meanings and subjects of calculation” (p. 186). Human, and especially
            women, computers were employed well into the 20th century, especially by the British and
            American governments, and many also became programmers of the early digital computers that would
            eventually replace them, remaking yet again the economy of labor and intelligence (Abbate, 2012;
            Ensmenger, 2010; Grier, 2007; Hicks, 2018; Light, 1999).
        </p>

        <p>This history also points to the fact that attempts to produce intelligent behavior in machines
            often run parallel to attempts to <em>make human behavior more machine-like</em>. From the
            disciplining
            of 19th-century factory workers’ bodies by the metronome to the automatic and unthinking
            execution of arithmetic that De Prony sought in his human computers, automation efforts often
            parallel the disciplining of human minds and bodies for the efficient execution of tasks.
            Indeed, Harry Collins has argued that machines can only appear to be intelligent in domains
            where people have already disciplined themselves to be sufficiently machine-like, as in the case
            of numerical calculation (Collins, 1992). This historical perspective invites a reconsideration
            of 21th and 21st century artificial intelligence as well. As anthropologist Lucy Suchman (2006)
            has proposed, artificial intelligence “works as a powerful disclosing agent for assumptions
            about the human” (p. 226). What behaviors are selected as exemplars of intelligence to be
            replicated by machinery? Whose cognitive labor is valued and devalued, displaced or replaced by
            the new economies of intelligence that surround modern digital computers or powerful machine
            learning systems?
        </p>

        <p>In fact, the most powerful and profitable artificial intelligences we have produced-those of
            today’s machine learning-exhibit a rather limited range of intelligent behavior. Overwhelmingly,
            machine learning systems are oriented towards one specific task: to make accurate predictions.
            Drawing on statistical techniques that date back to the mid-20th century, machine learning
            theorists aim to develop algorithms that take a huge amount of data as input to a neural
            network, and output a prediction rule or a classifier for the relevant domain in polynomial time
            (Valiant, 1984; Wald, 1947). Given this specific focus and particular history, many machine
            learning theorists would not situate their work in artificial intelligence in the traditional
            sense of the term at all. However, perhaps ironically, machine learning systems are billed as
            artificial intelligence by many public facing institutions, developers, and corporations, even
            though the original goal-to simulate human intelligent behavior-has not been a part of machine
            learning since neural networks were stripped of all but the barest reference to human brains in
            the 1960s (Jones, 2018). The name has stuck, in spite of the quite different definitions of
            ‘intelligence’ and ‘artificial’ at work there.
        </p>

        <p>Though machine learning systems have proven to be extremely powerful prediction engines, the new
            “artificial intelligence” has also been controversial. In 1997, John McCarthy penned a critique
            of DeepBlue in the wake of its victory over Kasparov. DeepBlue did not make use of neural
            networks, but it anticipated some debates that surround machine learning by deploying a brute
            force approach that would be unavailable to human players. McCarthy lamented that the human
            exemplar had not grounded its design-he didn’t want a program that could play chess well by any
            means, he wanted the recreation of human intelligence by automated means (1997). The critiques
            against inhuman artificial intelligences continue today. Some, like Noam Chomsky, have found the
            emphasis on predictive accuracy in machine learning epistemologically problematic and
            unsatisfying (Norvig, 2011). He doesn’t believe scientists should forfeit their concern with
            understanding and explanation in exchange for powerful data-driven prediction. Others, like
            Virginia Eubanks, Julia Angwin, Safiya Noble, and Cathy O’Neil, have noted how machine learning
            predictions can introduce ethically and socially problematic decision-making practices and
            reproduce or entrench bias and inequality (Angwin, 2016; Eubanks, 2017; Noble, 2018; O’Neil,
            2016). Both critiques draw in part from the fact that, especially in the case of
            non-interpretable systems, machine learning models can predict <em>that</em> something is the
            case, but
            do not offer any explanation of <em>why</em> it is the case, restricting the forms of knowledge,
            understanding, and accountability they afford. Some worry that today’s artificial intelligence
            doesn’t begin with human intelligence as a model, but that, in a sense, it may not <em>end</em>
            with
            human intelligence either.
        </p>

        <p>There isn’t a straightforward narrative of artificial intelligence from the 1950s until today.
            One important arc, however, is that the human exemplar, once the guide and the motivation for
            artificial intelligence research in its many postwar forms, has largely been displaced from the
            field, and so too have certain perspectives on what we are meant to <em>know</em> and
            <em>do</em> with intelligent
            machines.
        </p>

            <hr>

        <h4>Disclosure Statement</h4>
        <ul>
        <li>Stephanie Dick has no financial or non-financial disclosures to share for this article.</li>
        </ul>
        <h4>References</h4>
        <ul>
        <li>Abbate, J. (2017). <em>Recoding gender: Women’s changing participation in computing.</em>
            Cambridge, MA:
            MIT Press.
        </li>

        <li>Angwin, J., Larson, J., Mattu, S., & Kirchner, L. (2016). Machine bias. <em>Pro Publica</em>.
            <a
                href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.</a>
        </li>

        <li>Brock, D. (2018). Learning from artificial intelligence’s previous awakenings: The history of
            expert systems. <em>AI Magazine</em>, 39(3), 3–15. <a
                href="http://doi.org/10.1609/aimag.v39i3.2809">http://doi.org/10.1609/aimag.v39i3.2809</a>
        </li>

        <li>Collins, H. (1992). <em>Artificial experts: Social knowledge and intelligent machines</em>.
            Cambridge, MA:
            MIT Press.
        </li>

        <li>Crowther-Heyck, H. (2008). Defining the computer: Herbert Simon and the bureaucratic mind - part
            1. <em>IEEE Annals of the History of Computing, 30</em>(2), 42–51.
            <a href="https://doi.org/10.1109/MAHC.2008.19">https://doi.org/10.1109/MAHC.2008.19</a>
        </li>

        <li>Daston, L. (1994). Enlightenment calculations. <em>Critical Inquiry</em>, 21(1), 182–202.
            <a href="https://doi.org/10.1086/448745">https://doi.org/10.1086/448745</a>
        </li>

        <li>Ensmenger, N. (2010). <em>The computer boys take over: Computers, programmers, and the politics
                of
                technical expertise.</em> Cambridge, MA: MIT Press.
        </li>

        <li>Eubanks, V. (2017). <em>Automating inequality: How high-tech tools profile, police, and punish
                the
                poor.</em> New York: Macmillan.
        </li>

        <li>Feigenbaum, E. (1977). The art of artificial intelligence: Themes and case studies of knowledge
            engineering. Stanford Heuristics Programming Project Memo HPP-77-25.
        </li>

        <li>Floridi, L. (2016). <em>The fourth revolution: How the info sphere is reshaping human
                reality.</em>
            Oxford: Oxford University Press.
        </li>

        <li>Forsythe, D. (2002). <em>Studying those who study us: An anthropologist in the world of
                artificial
                intelligence.</em> Sanford, CA: Stanford University Press.
        </li>

        <li>Grier, D. (2007). <em>When computers were human.</em> Princeton, NJ: Princeton University Press.
        </li>

        <li>Heyck, H. (2005). <em>Herbert Simon: The bounds of reason in modern America.</em> Baltimore, ML:
            Johns
            Hopkins University Press.
        </li>

        <li>Hicks, Marie. (2017). <em>Programmed inequality: How Britain discarded women technologists and
                lost
                its edge in computing.</em> Cambridge, MA: MIT Press.
        </li>

        <li>Husbands, P., Holland, O., Wheeler, M. (Eds.). (2008). <em>The mechanical mind in history.</em>
            Cambridge,
            MA: MIT Press.
        </li>

        <li>Jones, M. (2018). How we became instrumentalists (again): Data positivism since World War II.
            <em>Historical Studies of the Natural Sciences, 48</em>(5), 673–684.
            <a
                href="https://doi.org/10.1525/HSNS.2018.48.5.673">https://doi.org/10.1525/HSNS.2018.48.5.673</a>
        </li>

        <li>Kline, R. (2011). Cybernetics, automata studies, and the Dartmouth conference on artificial
            intelligence. <em>IEEE Annals of the History of Computing 33</em>(4), 5–16.
            <a href="http://doi.org/10.1109/MAHC.2010.44">http://doi.org/10.1109/MAHC.2010.44</a>
        </li>

        <li>Kline, R. (2015). <em>The cybernetics moment: Or why we call our age the information age.</em>
            Baltimore,
            ML: Johns Hopkins University Press.
        </li>

        <li>Light, J. (1999). When computers were women. <em>Technology and Culture, 40</em>(3), 455–483.
            <a href="https://doi.org/10.1353/tech.1999.0128">https://doi.org/10.1353/tech.1999.0128</a>
        </li>

        <li>McCarthy, J., Minsky, M., Shannon, C. E., Rochester, N., & Dartmouth College. (1955). A proposal
            for the Dartmouth summer research project on artificial intelligence.
            <a href="https://doi.org/10.1609/aimag.v27i4.1904">https://doi.org/10.1609/aimag.v27i4.1904</a>
        </li>

        <li>McCarthy, J. (1997). AI as sport. <em>Science, 276</em>(5318), 1518–1519.
            <a
                href="https://doi.org/10.1126/science.276.5318.1518">https://doi.org/10.1126/science.276.5318.1518</a>
        </li>

        <li>McCorduck, P. <em>Machines who think.</em> Natick, MA: A K Peters.
        </li>

        <li>Newell, A., & Simon, H. (1972). <em>Human problem solving.</em> Oxford, England: Prentice-Hall.
        </li>

        <li>Noble, S. (2018). <em>Algorithms of oppression: How search engines reinforce racism.</em> New
            York, NY:
            New York University Press.
        </li>

        <li>Norvig, P. (2011). <em>On Chomsky and the two cultures of statistical learning.</em>
            <a href="http://norvig.com/chomsky.html">http://norvig.com/chomsky.html</a>
        </li>

        <li>O’Niel, C. (2017). <em>Weapons of math destruction: How big data increases inequality and
                threatens
                democracy.</em> New York, NY: Broadway Books.
        </li>

        <li>Schaffer, S. (1994). Babbage’s intelligence: Calculating engines and the factory system.
            <em>Critical
                Inquiry, 21</em>(1), 203–227. <a
                href="https://doi.org/10.1086/448746">https://doi.org/10.1086/448746</a>
        </li>

        <li>Suchman, L. (2006). <em>Human-machine reconfigurations.</em> Cambridge, UK: University of
            Cambridge Press.
        </li>

        <li>Tesauro, G. (1995). Temporal difference learning and TD-Gammon. <em>Communications of the ACM,
                38</em>(3),
            58–68. <a href="https://doi.org/10.1145/203330.203343">https://doi.org/10.1145/203330.203343</a>
        </li>

        <li>Valiant, L. (1984). A theory of the learnable. <em>Artificial Intelligence and Language
                Processing,
                27</em>(11), 1134–1142. <a
                href="https://doi.org/10.1145/800057.808710">https://doi.org/10.1145/800057.808710</a>
        </li>

        <li>Wald, A. (1947). <em>Sequential analysis.</em> New York: John Wiley and Sons.
        </li>

        <div contenteditable="false">
            <hr>
        </div>

        <li>©2019 Stephanie Dick. This article is licensed under a Creative Commons Attribution (CC BY 4.0)
            <a href="https://creativecommons.org/licenses/by/4.0/legalcode">International license</a>,
            except where otherwise indicated with respect to particular material
            included in the article.
        </li>
      </ul>

      </div>
    </div>


  </section>


{% endblock %}
